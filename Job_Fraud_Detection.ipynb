{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"id":"ooBVUDhyiRf-","executionInfo":{"status":"error","timestamp":1753132222158,"user_tz":-180,"elapsed":1927,"user":{"displayName":"Mazen Abdel-tawwab","userId":"15055954730560760871"}},"outputId":"56fe0277-9be5-4c91-8d51-d27a6a87eb39"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]},{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/content/drive/MyDrive/NMA Deep learning Project/fake_job_postings_processed.csv'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-4-4122011340.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Loading Dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/NMA Deep learning Project/fake_job_postings_processed.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_bad_lines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'skip'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/NMA Deep learning Project/fake_job_postings_processed.csv'"]}],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","import torch\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import TensorDataset, DataLoader\n","from sklearn.model_selection import train_test_split\n","\n","import re\n","from sklearn.utils import shuffle\n","from sklearn.metrics import classification_report\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Loading Dataset\n","df = pd.read_csv('/content/drive/MyDrive/NMA Deep learning Project/fake_job_postings_processed.csv', on_bad_lines='skip')\n"]},{"cell_type":"code","source":["fake = df[df[\"fraudulent\"] == 1]\n","real = df[df[\"fraudulent\"] == 0]\n","\n","# We undersample the majority class, this gives us less data but we don't have that much computational power anyways\n","n_minority = len(df[df['fraudulent'] == 1])\n","fake_sample = fake[[\"fraudulent\", \"text_processed\"]].sample(n_minority)\n","real_sample = real[[\"fraudulent\", \"text_processed\"]].sample(n_minority)\n","\n","sample = pd.concat((fake_sample, real_sample))\n","\n","text = [s for s in sample[\"text_processed\"]]\n","labels = np.array([int(l) for l in sample[\"fraudulent\"]])"],"metadata":{"id":"0oMxuFmHjkgG","executionInfo":{"status":"aborted","timestamp":1753132166982,"user_tz":-180,"elapsed":86,"user":{"displayName":"Mazen Abdel-tawwab","userId":"15055954730560760871"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print(device)"],"metadata":{"id":"8j-deXxFjuJ4","executionInfo":{"status":"aborted","timestamp":1753132166987,"user_tz":-180,"elapsed":90,"user":{"displayName":"Mazen Abdel-tawwab","userId":"15055954730560760871"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import BertTokenizer, BertModel\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","model = BertModel.from_pretrained(\"bert-base-uncased\").to(device)\n","\n","\n","def generate_embeddings(text: str) -> torch.Tensor:\n","    # Tokenize the text\n","    inputs = tokenizer(\n","        text,\n","        max_length=300,\n","        padding='max_length',\n","        truncation=True,\n","        return_tensors='pt'\n","    )\n","    inputs = {key: val.to(device) for key, val in inputs.items()}\n","\n","    # Generate embeddings\n","    with torch.no_grad():\n","        outputs = model(**inputs)\n","\n","    token_embeddings = outputs.last_hidden_state\n","    attention_mask = inputs['attention_mask'].unsqueeze(-1).expand(token_embeddings.size())\n","    return (token_embeddings * attention_mask).squeeze(0)"],"metadata":{"id":"K7YqwwOooWWE","executionInfo":{"status":"aborted","timestamp":1753132166988,"user_tz":-180,"elapsed":89,"user":{"displayName":"Mazen Abdel-tawwab","userId":"15055954730560760871"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["embeddings = np.array(\n","    [generate_embeddings(t).cpu().numpy() for t in text]\n",")"],"metadata":{"id":"fPloTfALp5hr","executionInfo":{"status":"aborted","timestamp":1753132167036,"user_tz":-180,"elapsed":128,"user":{"displayName":"Mazen Abdel-tawwab","userId":"15055954730560760871"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["embeddings.shape, labels.shape"],"metadata":{"id":"6wIZx_WvwNvO","executionInfo":{"status":"aborted","timestamp":1753132167040,"user_tz":-180,"elapsed":130,"user":{"displayName":"Mazen Abdel-tawwab","userId":"15055954730560760871"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train,X_test,y_train,y_test=train_test_split(embeddings, labels, test_size=0.2, random_state=0, stratify=labels)\n","train_data = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n","val_data = TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))\n","\n","# Batch size (this is an important hyperparameter)\n","batch_size = 40\n","\n","# dataloaders\n","# make sure to SHUFFLE your data\n","train_loader = DataLoader(\n","    train_data, shuffle=True, batch_size=batch_size, drop_last=True\n",")\n","val_loader = DataLoader(val_data, shuffle=True, batch_size=batch_size, drop_last=True)"],"metadata":{"id":"TEH7XInfjeKg","executionInfo":{"status":"aborted","timestamp":1753132167041,"user_tz":-180,"elapsed":30333,"user":{"displayName":"Mazen Abdel-tawwab","userId":"15055954730560760871"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class JobRNN(nn.Module):\n","    def __init__(self, n_layers, hidden_dim, embedding_dim, output_dim, drop_prob=0.1, device=device):\n","        super(JobRNN, self).__init__()\n","\n","        self.output_dim = output_dim\n","        self.hidden_dim = hidden_dim\n","        self.embedding_dim = embedding_dim\n","        self.n_layers = n_layers\n","        self.drop_prob = drop_prob\n","        self.device = device\n","\n","        self.lstm = nn.LSTM(\n","            input_size=self.embedding_dim,\n","            hidden_size=self.hidden_dim,\n","            num_layers=self.n_layers,\n","            batch_first=True,\n","            dropout=self.drop_prob\n","        )\n","\n","        self.dropout = nn.Dropout(p=self.drop_prob)\n","\n","        self.fc = nn.Linear(self.hidden_dim, self.output_dim)\n","        self.sig = nn.Sigmoid()\n","\n","    def forward(self, x, hidden):\n","        batch_size = x.size(0)\n","\n","        if x.dim() == 2: # We must be working with single vector embeddings\n","            x = x.unsqueeze(1)\n","        # Shape = [batch_size, max_len, embeddings_dim]\n","\n","        lstm_out, hidden = self.lstm(x, hidden)\n","        # shape = [batch_size, hidden_dim]\n","\n","        lstm_out = torch.mean(lstm_out, 1).contiguous()\n","\n","        out = self.dropout(lstm_out)\n","        out = self.fc(out)\n","        out = self.sig(out)\n","\n","        return out, hidden\n","\n","    def init_hidden(self, batch_size):\n","        h0 = torch.zeros(\n","            (self.n_layers, batch_size, self.hidden_dim)\n","        ).to(self.device)\n","        c0 = torch.zeros(\n","            (self.n_layers, batch_size, self.hidden_dim)\n","        ).to(self.device)\n","\n","        return (h0, c0)\n","\n","model = JobRNN(n_layers=2, hidden_dim=60, embedding_dim=768, output_dim=1, drop_prob=0.6)\n","model.to(device)\n","print(model)"],"metadata":{"id":"8Z4SV9eokkZN","executionInfo":{"status":"aborted","timestamp":1753132167048,"user_tz":-180,"elapsed":1,"user":{"displayName":"Mazen Abdel-tawwab","userId":"15055954730560760871"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lr = 0.01\n","criterion = nn.BCELoss()\n","optim = torch.optim.Adam(model.parameters(), lr=lr)\n","\n","\n","\n","def acc(pred, label):\n","    pred = torch.round(pred.squeeze())\n","    return torch.sum(pred == label.squeeze()).item()\n","\n","\n","epochs = 20\n","clip = 5  # maximum for |gradient|\n","valid_loss_min = np.inf  # initial loss value\n","\n","epoch_tr_loss, epoch_vl_loss = [], []\n","epoch_tr_acc, epoch_vl_acc = [], []\n","\n","\n","for epoch in range(epochs):\n","    train_losses = []\n","    train_acc = 0.0\n","    model.train()\n","\n","    for inputs, labels in train_loader:\n","        h = model.init_hidden(batch_size)\n","        h = tuple([each.data.to(device) for each in h])\n","\n","        inputs, labels = inputs.to(device), labels.to(device)\n","\n","        model.zero_grad()\n","\n","        output, h = model.forward(inputs, h)\n","        loss = criterion(output.squeeze(), labels.float())\n","\n","        loss.backward()\n","        train_losses.append(loss.item())\n","\n","        accuracy = acc(output, labels)\n","        train_acc += accuracy\n","\n","        # nn.utils.clip_grad_norm_(\n","        #     model.parameters(), clip\n","        # )  # helps prevent exploding gradient RNN/LSTM\n","        optim.step()\n","\n","    # validation set\n","    val_losses = []\n","    val_acc = 0.0\n","    model.eval()\n","\n","    for inputs, labels in val_loader:\n","        val_h = model.init_hidden(batch_size)\n","        val_h = tuple([each.data.to(device) for each in val_h])\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        output, h = model.forward(inputs, val_h)\n","        val_loss = criterion(output.squeeze(), labels.float())\n","        val_losses.append(val_loss.item())\n","        accuracy = acc(output, labels)\n","        val_acc += accuracy\n","\n","    epoch_train_loss = np.mean(train_losses)\n","    epoch_val_loss = np.mean(val_losses)\n","    epoch_train_acc = train_acc / len(train_loader.dataset)\n","    epoch_val_acc = val_acc / len(val_loader.dataset)\n","    epoch_tr_loss.append(epoch_train_loss)\n","    epoch_vl_loss.append(epoch_val_loss)\n","    epoch_tr_acc.append(epoch_train_acc)\n","    epoch_vl_acc.append(epoch_val_acc)\n","    print(f\"Epoch {epoch+1}\")\n","    print(f\"train_loss : {epoch_train_loss} val_loss : {epoch_val_loss}\")\n","    print(f\"train_accuracy : {epoch_train_acc*100} val_accuracy : {epoch_val_acc*100}\")\n","    if epoch_val_loss <= valid_loss_min:\n","        print(\n","            \"Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...\".format(\n","                valid_loss_min, epoch_val_loss\n","            )\n","        )\n","    # torch.save(model.state_dict(), '../working/state_dict.pt')\n","    valid_loss_min = epoch_val_loss\n","\n","    print(\"==\"*25)"],"metadata":{"id":"9t8uwwhNktlV","executionInfo":{"status":"aborted","timestamp":1753132167071,"user_tz":-180,"elapsed":3,"user":{"displayName":"Mazen Abdel-tawwab","userId":"15055954730560760871"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f\"\\nAverage Training Accuracy over {epochs} epochs: {np.mean(epoch_tr_acc)*100:.2f}%\")\n","print(f\"Average Validation Accuracy over {epochs} epochs: {np.mean(epoch_vl_acc)*100:.2f}%\")"],"metadata":{"id":"g7rwrH-Uw_sH","executionInfo":{"status":"aborted","timestamp":1753132167075,"user_tz":-180,"elapsed":1,"user":{"displayName":"Mazen Abdel-tawwab","userId":"15055954730560760871"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"g3rEiKmH0U8T","executionInfo":{"status":"aborted","timestamp":1753132167080,"user_tz":-180,"elapsed":1,"user":{"displayName":"Mazen Abdel-tawwab","userId":"15055954730560760871"}}},"execution_count":null,"outputs":[]}]}